#!/usr/bin/env python3
"""
VRPå®Ÿé¨“çµæœã‚’Databricks MLflowã«è¨˜éŒ²ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨æ–¹æ³•:
    python log_to_mlflow.py instances/f-n45-k4/results/experiment_results.json
    python log_to_mlflow.py instances/f-n135-k7/results/experiment_results.json
"""

import json
import sys
import os
from datetime import datetime
from typing import Dict, Any
import mlflow
try:
    from .visualization import create_comparison_visualization
except ImportError:
    # When running as script, use absolute import
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from visualization import create_comparison_visualization


def log_experiment_to_mlflow(experiment_json_path: str):
    """MLflowã«å®Ÿé¨“çµæœã‚’è¨˜éŒ²"""
    
    # 1. Databricks MLflowè¨­å®š
    print("Databricks MLflowã«æ¥ç¶šä¸­...")
    mlflow.set_tracking_uri("databricks")
    mlflow.set_experiment("/Shared/data_science/z_ogai/vrp-instances")
    
    # 2. JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å®Ÿé¨“çµæœã‚’èª­ã¿è¾¼ã¿
    print(f"å®Ÿé¨“çµæœã‚’èª­ã¿è¾¼ã¿ä¸­: {experiment_json_path}")
    with open(experiment_json_path, 'r', encoding='utf-8') as f:
        exp_results = json.load(f)
    
    # 3. Runåã¨èª¬æ˜æ–‡ã‚’ç”Ÿæˆ
    instance_name = exp_results["instance_info"]["name"].lower()
    experiment_id = exp_results["experiment_id"].split('_')[-1]  # exp003 -> 003
    run_name = f"{instance_name}_{experiment_id}"
    
    # æ—¥æœ¬èªã§ç°¡æ½”ãªèª¬æ˜
    gap = exp_results["gap_percentage"]
    if gap == 0:
        performance = "æœ€é©è§£ã¨å®Œå…¨ä¸€è‡´"
    elif gap <= 5:
        performance = f"å„ªç§€ãªè§£ï¼ˆã‚®ãƒ£ãƒƒãƒ— {gap:.2f}%ï¼‰"
    else:
        performance = f"è‰¯å¥½ãªè§£ï¼ˆã‚®ãƒ£ãƒƒãƒ— {gap:.2f}%ï¼‰"
    
    description = (
        f"OR-tools VRPã‚½ãƒ«ãƒãƒ¼ã«ã‚ˆã‚‹{exp_results['instance_info']['name']}ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®è§£æ³•å®Ÿé¨“ã€‚"
        f"Decimal.quantize()ã«ã‚ˆã‚‹æ­£ç¢ºãªè·é›¢è¨ˆç®—ã‚’å®Ÿè£…ã€‚{performance}ã‚’é”æˆã€‚"
    )
    
    print(f"MLflow Runã‚’é–‹å§‹: {run_name}")
    
    # 4. MLflowã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚’è¨˜éŒ²
    with mlflow.start_run(run_name=run_name, description=description):
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨˜éŒ²
        mlflow.log_param("model_type", exp_results["model_type"])
        mlflow.log_param("instance_name", exp_results["instance_info"]["name"])
        mlflow.log_param("dimension", exp_results["instance_info"]["dimension"])
        mlflow.log_param("num_vehicles", exp_results["instance_info"]["num_vehicles"])
        mlflow.log_param("capacity", exp_results["instance_info"]["capacity"])
        mlflow.log_param("distance_calculation", exp_results["preprocessing"]["distance_calculation"])
        
        # ã‚½ãƒ«ãƒãƒ¼è¨­å®š
        for key, value in exp_results["model_params"].items():
            mlflow.log_param(f"solver_{key}", value)
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
        mlflow.log_metric("optimal_cost", exp_results["optimal_cost"])
        mlflow.log_metric("solution_cost", exp_results["solution_cost"])
        mlflow.log_metric("gap_percentage", exp_results["gap_percentage"])
        mlflow.log_metric("solve_time_seconds", exp_results["solve_time_seconds"])
        mlflow.log_metric("num_routes", exp_results["num_routes"])
        
        # è¿½åŠ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        mlflow.log_metric("cost_efficiency", exp_results["optimal_cost"] / exp_results["solution_cost"])
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒï¼ˆf-n45-k4ã®ã¿ï¼‰
        if "benchmark_comparison" in exp_results:
            mlflow.log_metric("vs_int_improvement", exp_results["benchmark_comparison"]["vs_int_method"]["improvement"])
            mlflow.log_metric("vs_round_improvement", exp_results["benchmark_comparison"]["vs_round_method"]["improvement"])
        
        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£åˆ†æï¼ˆf-n135-k7ã®ã¿ï¼‰
        if "scalability_analysis" in exp_results:
            mlflow.log_metric("node_scaling_factor", exp_results["scalability_analysis"]["vs_f_n45_k4"]["node_scaling_factor"])
            mlflow.log_metric("cost_scaling_factor", exp_results["scalability_analysis"]["vs_f_n45_k4"]["cost_scaling_factor"])
        
        # ã‚¿ã‚°è¨­å®š
        mlflow.set_tag("problem_type", "CVRP")
        mlflow.set_tag("solver", "OR-tools")
        mlflow.set_tag("distance_method", "decimal_quantize")
        mlflow.set_tag("experiment_date", datetime.now().strftime("%Y-%m-%d"))
        
        if exp_results["gap_percentage"] == 0:
            mlflow.set_tag("performance", "optimal")
        elif exp_results["gap_percentage"] <= 5:
            mlflow.set_tag("performance", "excellent")
        elif exp_results["gap_percentage"] <= 10:
            mlflow.set_tag("performance", "good")
        else:
            mlflow.set_tag("performance", "fair")
        
        # ä¸Šä¸‹æ¯”è¼ƒå¯è¦–åŒ–ã‚’ä½œæˆ
        print("ä¸Šä¸‹æ¯”è¼ƒå¯è¦–åŒ–ã‚’ä½œæˆä¸­...")
        try:
            # ãƒ‘ã‚¹ã‹ã‚‰å®Ÿé¨“æƒ…å ±ã‚’æŠ½å‡º
            path_parts = experiment_json_path.split('/')
            instance_name = path_parts[1]  # f-n135-k7 or f-n45-k4
            
            #å®Ÿé¨“IDã‚’å–å¾—
            if len(path_parts) >= 4 and path_parts[3] != "experiment_results.json":
                # ä¾‹: instances/f-n135-k7/results/exp002/experiment_results.json
                exp_id = path_parts[3]  # exp002
            else:
                # ä¾‹: instances/f-n45-k4/results/experiment_results.json
                # experiment_results.jsonã‹ã‚‰å®Ÿé¨“IDã‚’æ¨å®š
                exp_id = exp_results["experiment_id"].split('_')[-1]  # f-n45-k4_exp003 -> exp003
            
            experiment_dir = f"instances/{instance_name}/experiments/{exp_id}"
            vrp_file = f"instances/{instance_name}/data/{instance_name}.vrp"
            sol_file = f"instances/{instance_name}/data/{instance_name}.sol"
            
            # ä¸Šä¸‹æ¯”è¼ƒå¯è¦–åŒ–ã‚’ä½œæˆ
            viz_path = create_comparison_visualization(
                experiment_dir=experiment_dir,
                experiment_id=exp_id,
                vrp_file=vrp_file,
                sol_file=sol_file,
                our_cost=exp_results["solution_cost"],
                optimal_cost=exp_results["optimal_cost"]
            )
            
            # å¯è¦–åŒ–ã‚’ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã¨ã—ã¦ä¿å­˜
            mlflow.log_artifact(viz_path, "visualizations")
            print(f"âœ… ä¸Šä¸‹æ¯”è¼ƒå¯è¦–åŒ–ã‚’ä½œæˆã—ã¾ã—ãŸ: {viz_path}")
            
        except Exception as e:
            print(f"âš ï¸  ä¸Šä¸‹æ¯”è¼ƒå¯è¦–åŒ–ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            # å¯è¦–åŒ–ãŒå¤±æ•—ã—ã¦ã‚‚MLflowã®è¨˜éŒ²ã¯ç¶šè¡Œ
        
        # ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã¨ã—ã¦JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        mlflow.log_artifact(experiment_json_path, "experiment_results")
        
        print("âœ… MLflowã¸ã®è¨˜éŒ²ãŒå®Œäº†ã—ã¾ã—ãŸ")
        print(f"Run ID: {mlflow.active_run().info.run_id}")


def main():
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ–¹æ³•: python log_to_mlflow.py <experiment_results.json_path>")
        print("ä¾‹:")
        print("  python log_to_mlflow.py instances/f-n45-k4/results/experiment_results.json")
        print("  python log_to_mlflow.py instances/f-n135-k7/results/experiment_results.json")
        sys.exit(1)
    
    experiment_json_path = sys.argv[1]
    
    if not os.path.exists(experiment_json_path):
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {experiment_json_path}")
        sys.exit(1)
    
    try:
        log_experiment_to_mlflow(experiment_json_path)
        print("ğŸ‰ MLflowè¨˜éŒ²ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()