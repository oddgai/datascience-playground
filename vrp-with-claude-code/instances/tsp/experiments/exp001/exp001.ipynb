{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSP Solver Comparison: OR-Tools vs MIP\n",
    "\n",
    "This experiment compares OR-Tools and MIP approaches for solving TSP instances extracted from VRP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport time\nimport json\nimport mlflow\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport japanize_matplotlib\n\nsys.path.append('../src')\nsys.path.append('../../../src')\nfrom tsp_utils import TSPDataExtractor\nfrom tsp_ortools import solve_tsp_with_ortools\nfrom tsp_mip import solve_tsp_with_mip"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract TSP Instances from VRP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract TSP instances of different sizes from TAI75A\nvrp_file = \"../../tai75a/data/tai75a.vrp\"\nextractor = TSPDataExtractor(vrp_file)\n\n# Create TSP instances of sizes 10, 15, 20\ntsp_sizes = [10, 15, 20]\ntsp_instances = {}\n\nfor size in tsp_sizes:\n    tsp_data = extractor.extract_tsp_subset(size, include_depot=True)\n    tsp_instances[f\"tsp{size}\"] = tsp_data\n    print(f\"Created TSP{size}: {tsp_data['dimension']} nodes\")\n    print(f\"  Distance matrix shape: {tsp_data['distance_matrix'].shape}\")\n    print(f\"  Original VRP nodes: {tsp_data['original_vrp_nodes']}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Solve with OR-Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortools_results = {}\n",
    "\n",
    "for name, tsp_data in tsp_instances.items():\n",
    "    print(f\"Solving {name} with OR-Tools...\")\n",
    "    result = solve_tsp_with_ortools(tsp_data, time_limit=60)\n",
    "    ortools_results[name] = result\n",
    "    print(f\"OR-Tools {name}: Cost={result.get('solution_cost', 'N/A')}, Time={result.get('solve_time_seconds', 'N/A'):.2f}s\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Solve with MIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mip_results = {}\n",
    "\n",
    "for name, tsp_data in tsp_instances.items():\n",
    "    print(f\"Solving {name} with MIP...\")\n",
    "    result = solve_tsp_with_mip(tsp_data, time_limit=60)\n",
    "    mip_results[name] = result\n",
    "    print(f\"MIP {name}: Cost={result.get('solution_cost', 'N/A')}, Time={result.get('solve_time_seconds', 'N/A'):.2f}s\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "print(\"=\" * 80)\n",
    "print(\"TSP Solver Comparison Results\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Instance':<10} {'OR-Tools Cost':<15} {'MIP Cost':<15} {'OR-Tools Time':<15} {'MIP Time':<15} {'Winner':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for name in tsp_instances.keys():\n",
    "    ortools_cost = ortools_results[name].get('solution_cost', float('inf'))\n",
    "    mip_cost = mip_results[name].get('solution_cost', float('inf'))\n",
    "    ortools_time = ortools_results[name].get('solve_time_seconds', 0)\n",
    "    mip_time = mip_results[name].get('solve_time_seconds', 0)\n",
    "    \n",
    "    if ortools_cost == float('inf') and mip_cost == float('inf'):\n",
    "        winner = \"None\"\n",
    "    elif ortools_cost == float('inf'):\n",
    "        winner = \"MIP\"\n",
    "    elif mip_cost == float('inf'):\n",
    "        winner = \"OR-Tools\"\n",
    "    elif ortools_cost < mip_cost:\n",
    "        winner = \"OR-Tools\"\n",
    "    elif mip_cost < ortools_cost:\n",
    "        winner = \"MIP\"\n",
    "    else:\n",
    "        winner = \"Tie\"\n",
    "    \n",
    "    print(f\"{name:<10} {ortools_cost:<15.1f} {mip_cost:<15.1f} {ortools_time:<15.2f} {mip_time:<15.2f} {winner:<10}\")\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'instance': name,\n",
    "        'size': int(name.replace('tsp', '')),\n",
    "        'ortools_cost': ortools_cost,\n",
    "        'mip_cost': mip_cost,\n",
    "        'ortools_time': ortools_time,\n",
    "        'mip_time': mip_time,\n",
    "        'winner': winner\n",
    "    })\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plots\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "sizes = [data['size'] for data in comparison_data]\n",
    "ortools_costs = [data['ortools_cost'] for data in comparison_data]\n",
    "mip_costs = [data['mip_cost'] for data in comparison_data]\n",
    "ortools_times = [data['ortools_time'] for data in comparison_data]\n",
    "mip_times = [data['mip_time'] for data in comparison_data]\n",
    "\n",
    "# Cost comparison\n",
    "ax1.plot(sizes, ortools_costs, 'o-', label='OR-Tools', color='blue', linewidth=2, markersize=8)\n",
    "ax1.plot(sizes, mip_costs, 's-', label='MIP', color='red', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('TSP Instance Size')\n",
    "ax1.set_ylabel('Solution Cost')\n",
    "ax1.set_title('Solution Quality Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Time comparison\n",
    "ax2.plot(sizes, ortools_times, 'o-', label='OR-Tools', color='blue', linewidth=2, markersize=8)\n",
    "ax2.plot(sizes, mip_times, 's-', label='MIP', color='red', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('TSP Instance Size')\n",
    "ax2.set_ylabel('Solve Time (seconds)')\n",
    "ax2.set_title('Computation Time Comparison')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Bar chart - costs\n",
    "x = np.arange(len(sizes))\n",
    "width = 0.35\n",
    "ax3.bar(x - width/2, ortools_costs, width, label='OR-Tools', color='blue', alpha=0.7)\n",
    "ax3.bar(x + width/2, mip_costs, width, label='MIP', color='red', alpha=0.7)\n",
    "ax3.set_xlabel('TSP Instance Size')\n",
    "ax3.set_ylabel('Solution Cost')\n",
    "ax3.set_title('Solution Cost Comparison')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels([f'TSP{s}' for s in sizes])\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Bar chart - times\n",
    "ax4.bar(x - width/2, ortools_times, width, label='OR-Tools', color='blue', alpha=0.7)\n",
    "ax4.bar(x + width/2, mip_times, width, label='MIP', color='red', alpha=0.7)\n",
    "ax4.set_xlabel('TSP Instance Size')\n",
    "ax4.set_ylabel('Solve Time (seconds)')\n",
    "ax4.set_title('Computation Time Comparison')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels([f'TSP{s}' for s in sizes])\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tsp_comparison.png', dpi=80, bbox_inches='tight', facecolor='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Log Results to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set MLflow tracking URI for Databricks\nimport os\nmlflow.set_tracking_uri(\"databricks\")\nmlflow.set_experiment(\"/Shared/data_science/z_ogai/tsp-experiments\")\n\n# Log OR-Tools results\nfor name, result in ortools_results.items():\n    with mlflow.start_run(run_name=f\"ortools_{name}\"):\n        # Log parameters\n        mlflow.log_param(\"solver_type\", \"OR-Tools\")\n        mlflow.log_param(\"instance_name\", name)\n        mlflow.log_param(\"instance_size\", result['instance_info']['dimension'])\n        \n        if 'model_params' in result:\n            for param, value in result['model_params'].items():\n                mlflow.log_param(param, value)\n        \n        # Log metrics\n        if 'solution_cost' in result:\n            mlflow.log_metric(\"solution_cost\", result['solution_cost'])\n        \n        if 'solve_time_seconds' in result:\n            mlflow.log_metric(\"solve_time_seconds\", result['solve_time_seconds'])\n        \n        if 'is_optimal' in result:\n            mlflow.log_metric(\"is_optimal\", 1 if result['is_optimal'] else 0)\n        \n        # Log result as artifact\n        with open(f\"{name}_ortools_result.json\", 'w') as f:\n            json.dump(result, f, indent=2, default=str)\n        mlflow.log_artifact(f\"{name}_ortools_result.json\")\n        os.remove(f\"{name}_ortools_result.json\")\n\nprint(\"OR-Tools results logged to Databricks MLflow\")\n\n# Log MIP results\nfor name, result in mip_results.items():\n    with mlflow.start_run(run_name=f\"mip_{name}\"):\n        # Log parameters\n        mlflow.log_param(\"solver_type\", \"MIP\")\n        mlflow.log_param(\"instance_name\", name)\n        mlflow.log_param(\"instance_size\", result['instance_info']['dimension'])\n        \n        if 'model_params' in result:\n            for param, value in result['model_params'].items():\n                mlflow.log_param(param, value)\n        \n        # Log metrics\n        if 'solution_cost' in result:\n            mlflow.log_metric(\"solution_cost\", result['solution_cost'])\n        \n        if 'solve_time_seconds' in result:\n            mlflow.log_metric(\"solve_time_seconds\", result['solve_time_seconds'])\n        \n        if 'is_optimal' in result:\n            mlflow.log_metric(\"is_optimal\", 1 if result['is_optimal'] else 0)\n        \n        # Log result as artifact\n        with open(f\"{name}_mip_result.json\", 'w') as f:\n            json.dump(result, f, indent=2, default=str)\n        mlflow.log_artifact(f\"{name}_mip_result.json\")\n        os.remove(f\"{name}_mip_result.json\")\n\nprint(\"MIP results logged to Databricks MLflow\")\n\n# Log comparison summary\nwith mlflow.start_run(run_name=\"tsp_comparison_summary\"):\n    mlflow.log_param(\"experiment_type\", \"TSP_Comparison\")\n    mlflow.log_param(\"num_instances\", len(tsp_instances))\n    mlflow.log_param(\"instance_sizes\", str(tsp_sizes))\n    \n    # Log aggregate metrics\n    ortools_wins = sum(1 for data in comparison_data if data['winner'] == 'OR-Tools')\n    mip_wins = sum(1 for data in comparison_data if data['winner'] == 'MIP')\n    ties = sum(1 for data in comparison_data if data['winner'] == 'Tie')\n    \n    mlflow.log_metric(\"ortools_wins\", ortools_wins)\n    mlflow.log_metric(\"mip_wins\", mip_wins)\n    mlflow.log_metric(\"ties\", ties)\n    \n    # Log comparison data\n    with open(\"comparison_summary.json\", 'w') as f:\n        json.dump(comparison_data, f, indent=2)\n    mlflow.log_artifact(\"comparison_summary.json\")\n    os.remove(\"comparison_summary.json\")\n    \n    # Log comparison plot\n    mlflow.log_artifact(\"tsp_comparison.png\")\n    os.remove(\"tsp_comparison.png\")\n\nprint(\"Comparison summary logged to Databricks MLflow\")\nprint(\"\\nTSP experiment completed! Check Databricks MLflow for results.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}